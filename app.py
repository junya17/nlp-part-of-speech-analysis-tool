from flask import Flask, request, jsonify, render_template
import spacy
import time

app = Flask(__name__)
nlp = spacy.load("en_core_web_sm")

# å“è©ã”ã¨ã®è‰²ã¨æ—¥æœ¬èªè¡¨è¨˜ï¼‹èª¬æ˜
POS_INFO = {
    "NOUN": {"color": "#007bff", "label": "åè©", "desc": "ç‰©ãƒ»äººãƒ»æ¦‚å¿µã‚’è¡¨ã™"},
    "VERB": {"color": "#ff4d4d", "label": "å‹•è©", "desc": "å‹•ä½œã‚„çŠ¶æ…‹ã‚’è¡¨ã™"},
    "ADJ": {"color": "#28a745", "label": "å½¢å®¹è©", "desc": "åè©ã‚’ä¿®é£¾ã™ã‚‹"},
    "ADV": {"color": "#ff8c00", "label": "å‰¯è©", "desc": "å‹•è©ã‚„å½¢å®¹è©ã‚’ä¿®é£¾ã™ã‚‹"},
    "PRON": {"color": "#6f42c1", "label": "ä»£åè©", "desc": "åè©ã®ä»£ã‚ã‚Šã«ãªã‚‹"},
    "DET": {"color": "#d63384", "label": "é™å®šè©", "desc": "åè©ã‚’ç‰¹å®šã™ã‚‹ï¼ˆthe, myï¼‰"},
    "ADP": {"color": "#ffc107", "label": "å‰ç½®è©", "desc": "åè©ã®å‰ã«ç½®ã‹ã‚Œã‚‹ï¼ˆin, onï¼‰"},
    "CONJ": {"color": "#17a2b8", "label": "æ¥ç¶šè©", "desc": "èªã‚„æ–‡ã‚’ã¤ãªã"},
    "NUM": {"color": "#6610f2", "label": "æ•°è©", "desc": "æ•°ã‚’è¡¨ã™"},
    "PART": {"color": "#20c997", "label": "åŠ©è©", "desc": "æ–‡ã®æ§‹é€ ã‚’è£œåŠ©ã™ã‚‹"},
    "AUX": {"color": "#e83e8c", "label": "åŠ©å‹•è©", "desc": "å‹•è©ã®æ„å‘³ã‚’è£œåŠ©ã™ã‚‹"},
    "PUNCT": {"color": "#6c757d", "label": "å¥èª­ç‚¹", "desc": "æ–‡ã®åŒºåˆ‡ã‚Šã‚’ç¤ºã™"},
    "SYM": {"color": "#fd7e14", "label": "è¨˜å·", "desc": "æ•°å¼ã‚„ç‰¹æ®Šè¨˜å·"},
    "X": {"color": "#dc3545", "label": "ãã®ä»–", "desc": "åˆ†é¡ã§ããªã„å˜èª"},
    "INTJ": {"color": "#17a2b8", "label": "æ„Ÿå‹•è©", "desc": "æ„Ÿæƒ…ã‚’è¡¨ã™"}
}

POS_CORRECTIONS = {
    "how": "ADV",  # ğŸ”¹ `SCONJ` ã‚’ `ADV` ã«ä¿®æ­£
    "what": "PRON",
    "who": "PRON",
    "which": "PRON",
    "whose": "PRON",
    "why": "ADV",
    "where": "ADV",
    "when": "ADV",
    "my": "DET",
    "your": "DET",
    "his": "DET",
    "her": "DET",
    "our": "DET",
    "their": "DET",
    "'s": "VERB"
}

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/analyze", methods=["POST"])
def analyze_text():
    data = request.json
    text = data.get("text", "").strip()

    print(f"Received text: {text}")
    doc = nlp(text)
    print(f"Processed text: {doc}")
    
    if not text:
        return jsonify({"error": "No text provided"}), 400

    doc = nlp(text)
    result = []
    word_count = len([token for token in doc if token.is_alpha])  # ğŸ”¹ è¨˜å·ã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å¤–ã—ãŸå˜èªæ•°

    # å“è©ã®ã‚«ã‚¦ãƒ³ãƒˆï¼ˆæ­£ã—ã `token.pos_` ãŒä¸€è‡´ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ï¼‰
    pos_counts = {"VERB": 0, "ADJ": 0, "ADV": 0, "CONJ": 0}

    for token in doc:
        word_lower = token.text.lower()  # å°æ–‡å­—ã«å¤‰æ›ã—ã¦æ¯”è¼ƒ
        pos = POS_CORRECTIONS.get(word_lower, token.pos_)  # ğŸ”¹ ä¿®æ­£ã•ã‚ŒãŸå“è©ã‚’é©ç”¨
        pos_data = POS_INFO.get(pos, {"label": pos, "color": "#000000", "desc": "èª¬æ˜ãªã—"})

        result.append({
            "word": token.text,
            "pos": pos_data["label"],  
            "color": pos_data["color"],
            "desc": pos_data["desc"]
        })
    
    return jsonify({"words": result})

if __name__ == "__main__":
    app.run(debug=True)
